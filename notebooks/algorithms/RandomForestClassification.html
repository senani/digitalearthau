


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Random forest classification &mdash; Digital Earth Australia 1.0.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/dea-favicon.ico"/>
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="Digital Earth Australia 1.0.0 documentation" href="../../index.html"/>
        <link rel="up" title="Algorithms" href="README.html"/>
        <link rel="next" title="Case Studies" href="../case_studies/README.html"/>
        <link rel="prev" title="ProduceFalseColourGeotiffs" href="ProduceFalseColourGeotiffs.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/DEA-logo-light.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                1.0.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../about/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Connect</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../connect/account.html">Register</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../connect/account.html#getting-help">Getting help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../connect/install.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../connect/nci_basics.html">Running DEA Manually</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../connect/jupyter.html">Jupyter Notebook</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../query/Getting Started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../query/List Products.html">List Products &amp; Measurements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../query/Loading Data.html">Loading data from the datacube</a></li>
</ul>
<p class="caption"><span class="caption-text">Notebook Gallery</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Notebook Gallery</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../products/README.html">Products</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="README.html">Algorithms</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="ProduceFalseColourGeotiffs.html">ProduceFalseColourGeotiffs</a></li>
<li class="toctree-l3"><a class="reference internal" href="ProduceFalseColourGeotiffs.html#This-is-the-bit-that-allows-you-to-call-in-a-stand-alone-python-file.">This is the bit that allows you to call in a stand alone python file.</a></li>
<li class="toctree-l3"><a class="reference internal" href="ProduceFalseColourGeotiffs.html#Extract-some-data-using-our-imported-function">Extract some data using our imported function</a></li>
<li class="toctree-l3"><a class="reference internal" href="ProduceFalseColourGeotiffs.html#Draw-a-false-colour-image-using-our-imported-function">Draw a false colour image using our imported function</a></li>
<li class="toctree-l3"><a class="reference internal" href="ProduceFalseColourGeotiffs.html#Export-the-three-bands-to-Geotiff-using-our-imported-function">Export the three bands to Geotiff using our imported function</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Random forest classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Set-up-analysis">Set up analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Import-modules">Import modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Functions-used-to-import-training-and-analysis-data">Functions used to import training and analysis data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Import-training-data-and-fit-model">Import training data and fit model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Import-analysis-data-and-classify">Import analysis data and classify</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Feature/band/variable-importance">Feature/band/variable importance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Plot-performance-of-model-by-parameter-values">Plot performance of model by parameter values</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Export-tree-diagrams">Export tree diagrams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../case_studies/README.html">Case Studies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../workflows/README.html">Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../README.html">Notebook Gallery Instructions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tags.html">Tagging Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../genindex.html">Tags Index</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Data Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../data/data.html">Surface Reflectance NBAR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data/data.html#surface-reflectance-nbar-t">Surface Reflectance NBAR-T</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data/data.html#pixel-quality">Pixel Quality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data/data.html#fractional-cover">Fractional Cover</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../internal/new_product.html">Creating New Product</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/release_process.html">Build a version</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/release_process.html#marking-it-stable">Marking it stable</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/git_best_practice.html">Git Best Practice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/release.html">Building a Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/orchestration.html">DEA Orchestration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/collection_management.html">Collection Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/collection_management.html#defining-collections">Defining Collections</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/goalkeeper_instructions.html">What’s a Goalkeeper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/requirements_met.html">DEA Smoke Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../internal/requirements_met.html#Data-load-examples">Data load examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Tags Index</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Digital Earth Australia</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Notebook Gallery</a> &raquo;</li>
        
          <li><a href="README.html">Algorithms</a> &raquo;</li>
        
      <li>Random forest classification</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/notebooks/algorithms/RandomForestClassification.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Random-forest-classification">
<h1>Random forest classification<a class="headerlink" href="#Random-forest-classification" title="Permalink to this headline">¶</a></h1>
<p><strong>What does this notebook do?</strong> This notebook classifies remote sensing
data using a random forest classifier model. Key features include being
able to efficiently import training data from a set of point, line or
polygon shapefiles (i.e. data is extracted from each shapefile
separately to avoid slow ‘dc.load’ on large areas), and allow flexible
and consistent selection of training and analysis data using import
functions (i.e. ensuring training data is consistent with analysis
data). The notebook exports geotiff classification outputs and a series
of evaluation figures to help fine-tune the classifier.</p>
<p><strong>Date:</strong> March 2018</p>
<p><strong>Author:</strong> Robbi Bishop-Taylor</p>
<p><strong>Tags:</strong></p>
</div>
<div class="section" id="Set-up-analysis">
<h1>Set up analysis<a class="headerlink" href="#Set-up-analysis" title="Permalink to this headline">¶</a></h1>
<p>Defines parameters used for analysis. Note: ‘data_func’ and
‘data_func_params’ need to be set at step 5 if changing remote sensing
data source used for training and classification</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>###########################################
# For HLTC-based mangrove classification: #
###########################################

# List of training files to import. Shapefiles can be either points, lines or polygons,
# but must be in the same projection system as the remote sensing dataset being analysed.
# Each file should cover a small enough spatial area so as to not slow dc.load function
# excessively (e.g. 100 x 100km max)
train_shps = [&quot;raw_data/train/training_data_mangrove.shp&quot;,
              &quot;raw_data/train/training_data_mangrove1.shp&quot;,
              &quot;raw_data/train/training_data_mangrove2.shp&quot;,
              &quot;raw_data/train/training_data_mangrove3.shp&quot;]

# Output path for classified geotiff
classification_output = &quot;output_data/classification_dc_mangrove.tif&quot;

# Optional dict to re-map training shapefile classes; useful for combining classes
# (&#39;3:2&#39; re-maps class 3 to class 2)
# train_reclass = {1:1, 2:2, 3:2, 4:2}
train_reclass = None

# Names of output classification classes
# classification_names = [&quot;mangrove&quot;, &quot;other&quot;]
classification_names = [&quot;mangrove&quot;, &quot;water&quot;, &quot;veg&quot;, &quot;other&quot;]

</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [82]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>##########################################
# For Tasseled cap-based classification: #
##########################################

# # List of training files to import. Shapefiles can be either points, lines or polygons,
# # but must be in the same projection system as the remote sensing dataset being analysed.
# # Each file should cover a small enough spatial area so as to not slow dc.load function
# # excessively (e.g. 100 x 100km max)
# train_shps = [&quot;raw_data/train/training_data_tasseledcap.shp&quot;]

# # Output path for classified geotiff
# classification_output = &quot;output_data/classification_dc_tasseledcap.tif&quot;

# # Optional dict to re-map training shapefile classes; useful for combining classes
# # (&#39;3:2&#39; re-maps class 3 to class 2)
# train_reclass = None

# # Names of output classification classes
# classification_names = [&quot;wetland&quot;, &quot;escarpment&quot;, &quot;plateau&quot;, &quot;other_wetland&quot;]

</pre></div>
</div>
</div>
</div>
<div class="section" id="Import-modules">
<h1>Import modules<a class="headerlink" href="#Import-modules" title="Permalink to this headline">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [64]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Load modules
import datacube
import fiona
import glob
import itertools
import matplotlib
import os
import rasterio
import warnings
import pandas as pd
import rasterio.features
import xarray as xr
import numpy as np
from collections import OrderedDict
from datacube.helpers import ga_pq_fuser
from datacube.storage import masking
from datacube.storage.masking import mask_invalid_data
from datacube.utils import geometry
from datacube.utils.geometry import CRS
from matplotlib import pyplot as plt
from osgeo import gdal
from os.path import splitext
from shapely.geometry import Point
from shapely.geometry import shape
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import scale
from sklearn.tree import export_graphviz
from sklearn.tree import _tree

# For nicer notebook plotting, hide warnings (comment out for real analysis)
warnings.filterwarnings(&#39;ignore&#39;)

# Set up datacube instance
dc = datacube.Datacube(app = &#39;Random forest classification&#39;)


def load_nbart(sensor, query):

    &#39;&#39;&#39;
    Loads nbart data for a sensor, mask by pq and filter terrain -999s.
    Modified from original function written by B Dunn (2017).

    :attr sensor: Landsat sensor to import (valid options: &#39;ls5&#39;, &#39;ls7&#39;, &#39;ls8&#39;)
    :attr query: complete datacube query used to import data

    :returns: xarray dataset matching query
    &#39;&#39;&#39;

    product_name = &#39;{}_{}_albers&#39;.format(sensor, &#39;nbart&#39;)
    print(&#39;Loading {}&#39;.format(product_name))
    ds = dc.load(product = product_name,
                 group_by = &#39;solar_day&#39;,
                 **query)

    if ds:

        print(&#39;Loaded {}&#39;.format(product_name))

        # Extract PQ data for masking
        mask_product = &#39;{}_{}_albers&#39;.format(sensor, &#39;pq&#39;)
        sensor_pq = dc.load(product = mask_product,
                            fuse_func = ga_pq_fuser,
                            group_by = &#39;solar_day&#39;,
                            **query)

        if sensor_pq:

            print(&#39;Making mask {}&#39;.format(mask_product))
            cloud_free = masking.make_mask(sensor_pq.pixelquality,
                                           cloud_acca = &#39;no_cloud&#39;,
                                           cloud_shadow_acca = &#39;no_cloud_shadow&#39;,
                                           cloud_shadow_fmask = &#39;no_cloud_shadow&#39;,
                                           cloud_fmask = &#39;no_cloud&#39;,
                                           blue_saturated = False,
                                           green_saturated = False,
                                           red_saturated = False,
                                           nir_saturated = False,
                                           swir1_saturated = False,
                                           swir2_saturated = False,
                                           contiguous = True)

            # Filter to remove clouds and -999 terrain issues
            ds = ds.where(cloud_free)
            ds = ds.where(ds != -999.0)

            # Add projection attributes
            ds.attrs[&#39;crs&#39;] = ds.crs
            ds.attrs[&#39;affine&#39;] = ds.affine
            ds.attrs[&#39;geo_transform&#39;] = ds.geobox.transform.to_gdal()
            ds.attrs[&#39;proj&#39;] = ds.geobox.crs.wkt
            print(&#39;Masked {} with {} and filtered &#39; \
                  &#39;terrain&#39;.format(product_name, mask_product))

        else:

            print(&#39;Did not mask {} with {}&#39;.format(product_name, mask_product))

    else:

        print (&#39;Did not load {}&#39;.format(product_name))

    if len(ds) &gt; 0:

        return ds

    else:

        return None


def rasterize_vector(input_data, cols, rows, geo_transform,
                     projection, field):

    &quot;&quot;&quot;
    Rasterize a vector file and return numpy array

    :attr input_data: input shapefile path or preloaded GDAL/OGR layer
    :attr cols: width of output array in columns
    :attr rows: height of output array in rows
    :attr geo_transform: geotransform for rasterization
    :attr projection: projection for rasterization
    :attr field: shapefile field to take values from

    :returns: a &#39;row x col&#39; array containg values from vector
    &quot;&quot;&quot;

    # If input data is a string, import as shapefile layer
    if isinstance(input_data, str):

        # Open vector with gdal
        data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)
        input_data = data_source.GetLayer(0)

    # Set up output raster
    driver = gdal.GetDriverByName(&#39;GTiff&#39;)  #&#39;MEM&#39;)  # In memory dataset
    target_ds = driver.Create(&#39;test.tif&#39;, cols, rows, 1, gdal.GDT_UInt16)
    target_ds.SetGeoTransform(geo_transform)
    target_ds.SetProjection(projection)

    # Rasterize shapefile and extract array
    gdal.RasterizeLayer(target_ds, [1], input_data, options=[&quot;ATTRIBUTE=&quot; + field])
    band = target_ds.GetRasterBand(1)
    out_array = band.ReadAsArray()
    target_ds = None

    return out_array


def write_geotiff(fname, data, geo_transform, projection,
                  nodata_val = 0, dtype = gdal.GDT_Float32):

    &quot;&quot;&quot;
    Create a single band GeoTIFF file with data from array.

    :attr fname: output file path
    :attr data: input array
    :attr geo_transform: geotransform for output raster
    :attr projection: projection for output raster
    :attr nodata_val: value to convert to nodata in output raster; default 0
    :attr dtype: value to convert to nodata in output raster; default gdal.GDT_Float32
    &quot;&quot;&quot;

    # Set up driver
    driver = gdal.GetDriverByName(&#39;GTiff&#39;)

    # Create raster of given size and projection
    rows, cols = data.shape
    dataset = driver.Create(fname, cols, rows, 1, dtype)
    dataset.SetGeoTransform(geo_transform)
    dataset.SetProjection(projection)

    # Write data to array and set nodata values
    band = dataset.GetRasterBand(1)
    band.WriteArray(data)
    band.SetNoDataValue(nodata_val)

    # Close file
    dataset = None


def tasseled_cap(sensor_data, sensor, tc_bands = [&#39;greenness&#39;, &#39;brightness&#39;, &#39;wetness&#39;],
                 drop = True):

    &quot;&quot;&quot;
    Computes tasseled cap wetness, greenness and brightness bands from a six
    band xarray dataset, and returns a new xarray dataset with old bands
    optionally dropped.

    Coefficients for demonstration purposes only; sourced from:
    Landsat 5: https://doi.org/10.1016/0034-4257(85)90102-6
    Landsat 7: https://doi.org/10.1080/01431160110106113
    Landsat 8: https://doi.org/10.1080/2150704X.2014.915434

    :attr sensor_data: input xarray dataset with six Landsat bands
    :attr tc_bands: list of tasseled cap bands to compute
    (valid options: &#39;wetness&#39;, &#39;greenness&#39;,&#39;brightness&#39;
    :attr sensor: Landsat sensor used for coefficient values
    (valid options: &#39;ls5&#39;, &#39;ls7&#39;, &#39;ls8&#39;)
    :attr drop: if &#39;drop = False&#39;, return all original Landsat bands

    :returns: xarray dataset with newly computed tasseled cap bands
    &quot;&quot;&quot;

    # Copy input dataset
    output_array = sensor_data.copy(deep = True)

    # Coefficients for each tasseled cap band
    wetness_coeff = {&#39;ls5&#39;:{&#39;blue&#39;:0.0315, &#39;green&#39;:0.2021, &#39;red&#39;:0.3102,
                            &#39;nir&#39;:0.1594, &#39;swir1&#39;:-0.6806, &#39;swir2&#39;:-0.6109},
                     &#39;ls7&#39;:{&#39;blue&#39;:0.2626, &#39;green&#39;:0.2141, &#39;red&#39;:0.0926,
                            &#39;nir&#39;:0.0656, &#39;swir1&#39;:-0.7629, &#39;swir2&#39;:-0.5388},
                     &#39;ls8&#39;:{&#39;blue&#39;:0.1511, &#39;green&#39;:0.1973, &#39;red&#39;:0.3283,
                            &#39;nir&#39;:0.3407, &#39;swir1&#39;:-0.7117, &#39;swir2&#39;:-0.4559}}

    greenness_coeff = {&#39;ls5&#39;:{&#39;blue&#39;:-0.1603, &#39;green&#39;:-0.2819, &#39;red&#39;:-0.4934,
                              &#39;nir&#39;:0.7940, &#39;swir1&#39;:-0.0002, &#39;swir2&#39;:-0.1446},
                       &#39;ls7&#39;:{&#39;blue&#39;:-0.3344, &#39;green&#39;:-0.3544, &#39;red&#39;:-0.4556,
                              &#39;nir&#39;:0.6966, &#39;swir1&#39;:-0.0242, &#39;swir2&#39;:-0.2630},
                       &#39;ls8&#39;:{&#39;blue&#39;:-0.2941, &#39;green&#39;:-0.2430, &#39;red&#39;:-0.5424,
                              &#39;nir&#39;:0.7276, &#39;swir1&#39;:-0.0713, &#39;swir2&#39;:-0.1608}}

    brightness_coeff = {&#39;ls5&#39;:{&#39;blue&#39;:0.2043, &#39;green&#39;:0.4158, &#39;red&#39;:0.5524,
                               &#39;nir&#39;:0.5741, &#39;swir1&#39;:0.3124, &#39;swir2&#39;:0.2303},
                        &#39;ls7&#39;:{&#39;blue&#39;:0.3561, &#39;green&#39;:0.3972, &#39;red&#39;:0.3904,
                               &#39;nir&#39;:0.6966, &#39;swir1&#39;:0.2286, &#39;swir2&#39;:0.1596},
                        &#39;ls8&#39;:{&#39;blue&#39;:0.3029, &#39;green&#39;:0.2786, &#39;red&#39;:0.4733,
                               &#39;nir&#39;:0.5599, &#39;swir1&#39;:0.508, &#39;swir2&#39;:0.1872}}

    # Dict to use correct coefficients for each tasseled cap band
    analysis_coefficient = {&#39;wetness&#39;: wetness_coeff,
                            &#39;greenness&#39;: greenness_coeff,
                            &#39;brightness&#39;: brightness_coeff}

    # For each band, compute tasseled cap band and add to output dataset
    for tc_band in tc_bands:

        # Create xarray of coefficient values used to multiply each band of input
        coeff = xr.Dataset(analysis_coefficient[tc_band][sensor])
        sensor_coeff = sensor_data * coeff

        # Sum all bands
        output_array[tc_band] = sensor_coeff.blue + sensor_coeff.green + \
                                sensor_coeff.red + sensor_coeff.nir + \
                                sensor_coeff.swir1 + sensor_coeff.swir2

    # If drop = True, remove original bands
    if drop:

        bands_to_drop = list(sensor_data.data_vars)
        output_array = output_array.drop(bands_to_drop)

    return(output_array)


def layer_extent(layer):

    &quot;&quot;&quot;
    Computes min and max extents for GDAL layer features. Compared to
    built-in &quot;.GetExtent&quot; that always returns unfiltered extents, this
    allows you to compute extents of features within filtered layers
    (e.g. layers filtered with &#39;SetAttributeFilter&#39;). Works for point,
    line and polygon features, returning the extent of feature centroids
    for lines and polygon datasets.
    &quot;&quot;&quot;

    # Extract tuples of x, y, z coordinates for each point feature
    point_coords = [feature.geometry().Centroid().GetPoint() for feature in layer]

    # Compute mins and maxes across points for each tuple element
    max_x, max_y, max_z = map(min, zip(*point_coords))
    min_x, min_y, min_z = map(max, zip(*point_coords))

    return  min_x, max_x, min_y, max_y


def randomforest_train(train_shps, train_field, data_func, data_func_params = {},
                       classifier_params = {}, train_reclass = None):

    &#39;&#39;&#39;
    Extracts training data from xarray dataset for multiple training shapefiles.
    Loops through each each training shapefile, using shapefile extent for spatial
    query. Outputs a trained classifier object and training label and data arrays.

    :attr train_shps: list of training shapefile paths to import. Each file
    should cover a small enough spatial area so as to not slow dc.load function
    excessively (e.g. 100 x 100km max)
    :attr train_field: shapefile field containing classification class
    :attr data_func: function to import xarray data for each shapefile. Should return
    an xarray dataset with &#39;geo_transform&#39; and &#39;proj&#39; attributes
    :attr data_func_params: optional dict of dc.load query inputs. Useful for defining
    time query for temporal datasets (spatial queries are set automatically from shapefiles)
    :attr classifier_params: optional dict of parameters for training random forest
    :attr train_reclass: optional dict of from:to pairs to re-map shapefile field classes.
    Useful for simplifying multiple classes into a simpler set of classes

    :returns: trained classifier
    :returns: array of training labels
    :returns: array of training data
    &#39;&#39;&#39;

    # Output training label and pixel arrays
    training_labels_list = list()
    training_samples_list = list()

    # For each shapefile, extract datacube data using extent of points
    # and add resulting spectral data and labels to list of arrays
    for train_shp in train_shps:

        print(&quot;Importing training data from {}:&quot;.format(train_shp))

        try:

            # Open vector of training points with gdal
            data_source = gdal.OpenEx(train_shp, gdal.OF_VECTOR)
            layer = data_source.GetLayer(0)

            # Compute extents and generate spatial query
            xmin, xmax, ymin, ymax = layer_extent(layer)
            query_train = {&#39;x&#39;: (xmin + 2000, xmax - 2000),
                           &#39;y&#39;: (ymin + 2000, ymax - 2000),
                           &#39;crs&#39;: &#39;EPSG:3577&#39;,
                           **data_func_params}
            print(query_train)

            # Import data  as xarray and extract projection/transform data
            training_xarray = data_func(query_train)
            geo_transform_train = training_xarray.geo_transform
            proj_train = training_xarray.proj

            # Covert to array and rearrange dimension order
            bands_array_train = training_xarray.to_array().values
            bands_array_train = np.einsum(&#39;bxy-&gt;xyb&#39;, bands_array_train)
            rows_train, cols_train, bands_n_train = bands_array_train.shape

            # Import training data shapefiles and convert to matching raster pixels
            training_shapefile = train_shp
            training_pixels = rasterize_vector(layer, cols_train, rows_train,
                                               geo_transform_train, proj_train,
                                               field = train_field)

            # Extract matching image sample data for each labelled pixel location
            is_train = np.nonzero(training_pixels)
            training_labels = training_pixels[is_train]
            training_samples = bands_array_train[is_train]

            # Remove nans from training samples
            training_labels = training_labels[~np.isnan(training_samples).any(axis=1)]
            training_samples = training_samples[~np.isnan(training_samples).any(axis=1)]

            # Append outputs
            training_labels_list.append(training_labels)
            training_samples_list.append(training_samples)

        except AttributeError:

            print(&quot;  Skipping training data from {}; check file path&quot;.format(train_shp))

    # Combine polygon training data
    training_labels = np.concatenate(training_labels_list, axis=0)
    training_samples = np.concatenate(training_samples_list, axis=0)

    # Optionally re-map classes prior to classification training
    if train_reclass:

        # For each class in training labels, re-map to new values using train_reclass
        training_labels[:] = [train_reclass[label] for label in training_labels]

    # Set up classifier and train on training sample data and labels
    # Options for tuning: https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/
    print(&quot;\nTraining random forest classifier...&quot;)
    classifier = RandomForestClassifier(**classifier_params)
    classifier.fit(training_samples, training_labels)
    print(&quot;Model trained on {0} bands and &quot;
          &quot;{1} training samples&quot;.format(training_samples.shape[1],
                                        str(len(training_samples))))

    return classifier, training_labels, training_samples


def randomforest_classify(classifier, analysis_data, classification_output, class_prob = False):

    &#39;&#39;&#39;
    Performs classification of xarray dataset using pre-trained random forest classifier,
    and export classified output to a geotiff. Optionally, also export a predicted class
    probability raster (i.e. indicating fraction of samples of the predicted class in a leaf)

    :attr classifier: random forest classifier generated using randomforest_train
    :attr analysis_data: xarray dataset with &#39;geo_transform&#39; and &#39;proj&#39; attributes
    and the same number of bands as data used to train classifier
    :attr classification_output: file path to output geotiff classification
    :attr class_prob: if True, compute predicted class probability and export to
    geotiff suffixed with &quot;_prob.tif&quot;

    :returns: classified array and (optional) classification probability array
    &#39;&#39;&#39;

    geo_transform = analysis_data.geo_transform
    proj = analysis_data.proj

    # Covert to array and rearrange dimension order
    analysis_array = analysis_data.to_array().values
    analysis_array = np.einsum(&#39;bxy-&gt;xyb&#39;, analysis_array)
    rows, cols, bands_n = analysis_array.shape
    print(&quot;Data to classify:\n  Rows: {0}\n  Columns: {1}\n  Bands: {2}&quot;.format(rows, cols, bands_n))

    # Remove nodata and return flattened &#39;pixel x bands&#39; array
    input_nodata = np.isnan(analysis_array).any(axis = 2)
    flat_pixels = analysis_array[~input_nodata]

    # Run classification
    print(&quot;\nClassification processing...&quot;)
    result = classifier.predict(flat_pixels)

    # Restore 2D array by assigning flattened output to empty array
    classification = np.zeros((rows, cols))
    classification[~input_nodata] = result

    # Nodata removed
    print(&quot;  {} nodata cells removed&quot;.format(str(np.sum(classification == 0))))

    # Export to file
    write_geotiff(classification_output,
                  data = classification,
                  geo_transform = geo_transform,
                  projection = proj,
                  nodata_val = 0,
                  dtype = gdal.GDT_Byte)
    print(&quot;  Classification exported&quot;)

    # If requested, export classification probability:
    if class_prob:

        # Compute predicted class probability (fraction of samples of same class in a leaf)
        # Use max to return only highest probability (the one that determined output class)
        print(&quot;\nClass probability processing...&quot;)
        result_prob = classifier.predict_proba((flat_pixels))
        result_prob = np.max(result_prob, axis = 1) * 100.0

        # Restore 2D array by assigning flattened output to empty array
        classification_prob = np.zeros((rows, cols))
        classification_prob[~input_nodata] = result_prob

        # Export to file
        write_geotiff(splitext(classification_output)[0] + &quot;_prob.tif&quot;,
                      data = classification_prob,
                      geo_transform = geo_transform,
                      projection = proj,
                      nodata_val = -999,
                      dtype = gdal.GDT_Byte)
        print(&quot;  Class probability exported&quot;)

        return classification, classification_prob

    else:

        return classification, None


def randomforest_eval(training_labels, training_samples, classifier_scenario,
                      output_path, max_estimators = 100):

    &quot;&quot;&quot;
    Takes a set of training labels and training samples, and plots OOB error against
    a range of classifier parameters to explore how parameters affect classification.

    :attr training_labels: an (X, ) array of training labels
    :attr training_samples: an (X, B) array of training sample data
    :attr classifier_scenario: dict of classifier scenarios to plot
    :attr output_path: output path for plot of OOB error by scenario
    :attr max_estimators: max number of estimators to plot on x-axis (default = 100)
    &quot;&quot;&quot;

    # Map a classifier name to a list of (&lt;n_estimators&gt;, &lt;error rate&gt;) pairs.
    error_rate = OrderedDict((label, []) for label, _ in classifier_scenario)

    # Range of `n_estimators` values to explore.
    min_estimators = 1

    for label, clf in classifier_scenario:
        for i in range(min_estimators, max_estimators + 1):
            clf.set_params(n_estimators = i)
            clf.fit(train_samp, train_lab)

            # Record the OOB error for each `n_estimators=i` setting.
            oob_error = 1 - clf.oob_score_
            error_rate[label].append((i, oob_error))

    # Generate the &quot;OOB error rate&quot; vs. &quot;n_estimators&quot; plot.
    for label, clf_err in error_rate.items():
        xs, ys = zip(*clf_err)
        plt.plot(xs, ys, label = label)

    # Plot and save output as figure
    plt.xlim(min_estimators, max_estimators)
    plt.xlabel(&quot;n_estimators&quot;)
    plt.ylabel(&quot;OOB error rate&quot;)
    plt.legend(loc = &quot;upper right&quot;)
    plt.yscale(&#39;log&#39;)
    plt.savefig(output_path, bbox_inches = &#39;tight&#39;)
    plt.show()

</pre></div>
</div>
</div>
</div>
<div class="section" id="Functions-used-to-import-training-and-analysis-data">
<h1>Functions used to import training and analysis data<a class="headerlink" href="#Functions-used-to-import-training-and-analysis-data" title="Permalink to this headline">¶</a></h1>
<p>Both of these functions import datacube data using a query, and return
an xarray dataset with multiple bands/variables and ‘geo_transform’ and
‘proj’ attributes. This format is required as an input to both
‘randomforest_train’ and ‘randomforest_classify’ to ensure that both
training and analysis data are consistent.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [65]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>def hltc_import(query):

    &quot;&quot;&quot;
    Imports high and low composite data for a given spatial query, and
    return an xarray dataset with &#39;geo_transform&#39; and &#39;proj&#39; attributes

    :attr query: spatial query for datacube.load()
    :returns: xarray dataset with &#39;geo_transform&#39; and &#39;proj&#39; attributes
    &quot;&quot;&quot;

    # Import data
    low_tide = dc.load(product = &#39;low_tide_comp_20p&#39;, **query)
    high_tide = dc.load(product = &#39;high_tide_comp_20p&#39;, **query)

    # Rename variables in each high/low composite so datasets can be merged
    data_vars = list(low_tide.data_vars)
    low_tide.rename({var: &quot;lt_&quot; + var for var in data_vars}, inplace = True)
    high_tide.rename({var: &quot;ht_&quot; + var for var in data_vars}, inplace = True)

    # Combine into one dataset
    output_xarray = xr.auto_combine([low_tide, high_tide]).isel(time = 0)

    # Set attributes
    output_xarray.attrs[&#39;proj&#39;] = low_tide.geobox.crs.wkt
    output_xarray.attrs[&#39;geo_transform&#39;] = low_tide.geobox.transform.to_gdal()

    return output_xarray


def tc_import(query):

    &#39;&#39;&#39;
    Wrapper around load_nbart and tasseled cap to return an xarray dataset with
    &#39;geo_transform&#39; and &#39;proj&#39; attributes

    :attr query: query for datacube call; for training, supply only
    non-spatial queries as spatial are generated from training data
    :returns: xarray dataset with &#39;geo_transform&#39; and &#39;proj&#39; attributes
    &#39;&#39;&#39;

    # Import cleaned Landsat bands data
    nbar_example = load_nbart(&#39;ls8&#39;, query)

    # Compute tasseled cap indices and take median of multiple timesteps
    output_xarray = tasseled_cap(sensor_data = nbar_example,
                                 sensor = &#39;ls8&#39;,
                                 drop = True).median(&quot;time&quot;, keep_attrs = True)

    return output_xarray

</pre></div>
</div>
</div>
</div>
<div class="section" id="Import-training-data-and-fit-model">
<h1>Import training data and fit model<a class="headerlink" href="#Import-training-data-and-fit-model" title="Permalink to this headline">¶</a></h1>
<p>Uses ‘randomforest_train’ to extract training data from potentially
multiple training shapefiles, and returns a trained classifier (and
optionally, training label and training sample arrays)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [66]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Dict of classifier parameters
classifier_params = {&#39;n_jobs&#39;: -1,
                     &#39;n_estimators&#39;: 100,
                     &#39;max_features&#39;: &quot;auto&quot;,
                     &#39;min_samples_leaf&#39;: 1,
                     &#39;oob_score&#39;: True }

# Set data function used to import data and optional parameters (e.g. time for temporal data).
# For example, &#39;tc_import&#39; needs an additional &#39;time&#39; query as it draws on Landsat time-series
# data, while &#39;hltc_import&#39; uses high-low tide composites that have no temporal dimension
data_func = hltc_import
data_func_params = {}
# data_func = tc_import
# data_func_params = {&#39;time&#39;: (&#39;2017-03-01&#39;, &#39;2017-06-28&#39;)}

# Extract training data for each training shapefile and train classifier
classifier, train_lab, train_samp = randomforest_train(train_shps = train_shps,
                                                       train_field = &quot;class&quot;,
                                                       data_func = data_func,
                                                       data_func_params = data_func_params,
                                                       classifier_params = classifier_params,
                                                       train_reclass = train_reclass)

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Importing training data from raw_data/train/training_data_mangrove.shp:
{&#39;x&#39;: (-96195.37617727809, -159027.5762163046), &#39;y&#39;: (-1293746.4245350992, -1349139.5474466463), &#39;crs&#39;: &#39;EPSG:3577&#39;}
Importing training data from raw_data/train/training_data_mangrove1.shp:
{&#39;x&#39;: (-248822.2628674942, -269215.4139100249), &#39;y&#39;: (-1570514.9681683218, -1590991.9822624666), &#39;crs&#39;: &#39;EPSG:3577&#39;}
Importing training data from raw_data/train/training_data_mangrove2.shp:
{&#39;x&#39;: (-216770.8137106544, -237605.05985552695), &#39;y&#39;: (-1630468.8744477245, -1646265.5913745423), &#39;crs&#39;: &#39;EPSG:3577&#39;}
Importing training data from raw_data/train/training_data_mangrove3.shp:
{&#39;x&#39;: (-699112.9082409441, -724416.739027992), &#39;y&#39;: (-1612261.936734104, -1634952.3249203684), &#39;crs&#39;: &#39;EPSG:3577&#39;}

Training random forest classifier...
Model trained on 12 bands and 352 training samples
</pre></div></div>
</div>
</div>
<div class="section" id="Import-analysis-data-and-classify">
<h1>Import analysis data and classify<a class="headerlink" href="#Import-analysis-data-and-classify" title="Permalink to this headline">¶</a></h1>
<p>Classifies and exports an analysis dataset using a previously trained
random forest classifier, provided this dataset has the same number of
bands/variables as the data used to train the classifier. Using the same
data function used to train the classifier (e.g. ‘data_func’ previously
defined as either ‘tc_import’ or ‘hltc_import’) will ensure this is
the case. By setting ‘class_prob = True’, can optionally export a
geotiff of predicted class probabilities in addition to the
classification output.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [109]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Set up analysis data query
lat_point, lon_point, buffer = -12.5798399926, 130.782907919, 50000
x, y = geometry.point(lon_point, lat_point, CRS(&#39;WGS84&#39;)).to_crs(CRS(&#39;EPSG:3577&#39;)).points[0]
query = {&#39;x&#39;: (x - buffer, x + buffer),
         &#39;y&#39;: (y - buffer, y + buffer),
         &#39;crs&#39;: &#39;EPSG:3577&#39;,
          **data_func_params}

# Load data from datacube
analysis_xarray = data_func(query)

# Run classification and export to file
class_array, prob_array = randomforest_classify(classifier = classifier,
                                                analysis_data = analysis_xarray,
                                                classification_output = classification_output,
                                                class_prob = True)

# Plot output classification
class_xarray = xr.DataArray(class_array,
                   coords = [analysis_xarray.y, analysis_xarray.x],
                   dims = [&#39;y&#39;, &#39;x&#39;],
                   name = &quot;Classification output&quot;)
class_xarray.plot(levels = list(np.unique(class_array)) + [len(np.unique(class_array)) + 1],
                               figsize = (8, 8))

# Plot predicted class probability (proportion of trees agreeing with classification)
plt.plot()
prob_xarray = xr.DataArray(prob_array,
                           coords = [analysis_xarray.y, analysis_xarray.x],
                           dims = [&#39;y&#39;, &#39;x&#39;],
                           name = &quot;Predicted class probability&quot;)
prob_xarray.plot(cmap = &quot;plasma_r&quot;, figsize = (8, 8),
                 vmin = np.percentile(prob_array, 3),
                 vmax = np.percentile(prob_array, 97))

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Data to classify:
  Rows: 4001
  Columns: 4001
  Bands: 12

Classification processing...
  0 nodata cells removed
  Classification exported

Class probability processing...
  Class probability exported
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[109]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.collections.QuadMesh at 0x7fd995266a58&gt;
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_11_2.png" src="../../_images/notebooks_algorithms_RandomForestClassification_11_2.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_11_3.png" src="../../_images/notebooks_algorithms_RandomForestClassification_11_3.png" />
</div>
</div>
</div>
<div class="section" id="Feature/band/variable-importance">
<h1>Feature/band/variable importance<a class="headerlink" href="#Feature/band/variable-importance" title="Permalink to this headline">¶</a></h1>
<p>Extract classifier estimates of the relative importance of each
band/variable for training the classifier. Useful for potentially
selecting a subset of input bands/variables for model
training/classification (i.e. optimising feature space)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [79]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>#  Extract feature importances from trained classifier
importance = classifier.feature_importances_
importance_df = pd.DataFrame({&#39;variable&#39;: analysis_xarray.data_vars,
                              &#39;importance&#39;: importance})
importance_df.set_index(&quot;variable&quot;, inplace = True)
importance_df.plot.bar(title = &quot;Variable importance (global)&quot;)
display(importance_df)

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>importance</th>
    </tr>
    <tr>
      <th>variable</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>lt_blue</th>
      <td>0.078837</td>
    </tr>
    <tr>
      <th>lt_green</th>
      <td>0.047461</td>
    </tr>
    <tr>
      <th>lt_red</th>
      <td>0.041073</td>
    </tr>
    <tr>
      <th>lt_nir</th>
      <td>0.070420</td>
    </tr>
    <tr>
      <th>lt_swir1</th>
      <td>0.065832</td>
    </tr>
    <tr>
      <th>lt_swir2</th>
      <td>0.124902</td>
    </tr>
    <tr>
      <th>ht_blue</th>
      <td>0.065044</td>
    </tr>
    <tr>
      <th>ht_green</th>
      <td>0.058947</td>
    </tr>
    <tr>
      <th>ht_red</th>
      <td>0.038579</td>
    </tr>
    <tr>
      <th>ht_nir</th>
      <td>0.083995</td>
    </tr>
    <tr>
      <th>ht_swir1</th>
      <td>0.137795</td>
    </tr>
    <tr>
      <th>ht_swir2</th>
      <td>0.187116</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_13_1.png" src="../../_images/notebooks_algorithms_RandomForestClassification_13_1.png" />
</div>
</div>
</div>
<div class="section" id="Plot-performance-of-model-by-parameter-values">
<h1>Plot performance of model by parameter values<a class="headerlink" href="#Plot-performance-of-model-by-parameter-values" title="Permalink to this headline">¶</a></h1>
<p>Random forest classifiers contain many modifiable parameters that can
strongly affect the performance of the model. This section evaluates the
effect of these parameters by plotting out-of-bag (OOB) error for a set
of classifier parameter scenarios, and exports the resulting plots to
file.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [80]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># NOTE: Setting the `warm_start` construction parameter to `True` disables
# support for parallelized ensembles but is necessary for tracking the OOB
# error trajectory during training.

# Test effect of max features
classifier_scenario1 = [(&quot;max_features = &#39;sqrt&#39;&quot;,
                        RandomForestClassifier(warm_start = True,
                                               oob_score = True,
                                               max_features = &quot;sqrt&quot;)),
                       (&quot;max_features = &#39;log2&#39;&quot;,
                        RandomForestClassifier(warm_start = True,
                                               oob_score = True,
                                               max_features = &quot;log2&quot;)),

                       (&quot;max_features = &#39;0.1&#39;&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_features = 0.1,
                                               oob_score = True)),

                       (&quot;max_features = &#39;0.5&#39;&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_features = 0.5,
                                               oob_score = True)),
                       (&quot;max_features = None&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_features = None,
                                               oob_score = True))]

# Test effect of minimum samples per leaf
classifier_scenario2 = [(&quot;Leaf = 1&quot;,
                         RandomForestClassifier(warm_start = True,
                                                min_samples_leaf = 1,
                                                oob_score = True)),
                       (&quot;Leaf = 10&quot;,
                        RandomForestClassifier(warm_start = True,
                                               min_samples_leaf = 10,
                                               oob_score = True)),
                       (&quot;Leaf = 20&quot;,
                        RandomForestClassifier(warm_start = True,
                                               min_samples_leaf = 20,
                                               oob_score = True)),
                       (&quot;Leaf = 40&quot;,
                        RandomForestClassifier(warm_start = True,
                                               min_samples_leaf = 40,
                                               oob_score = True))]

# Test effect of max depth
classifier_scenario3 = [(&quot;Max depth = 1&quot;,
                         RandomForestClassifier(warm_start = True,
                                                max_depth = 1,
                                                oob_score = True)),
                       (&quot;Max depth = 2&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_depth = 2,
                                               oob_score = True)),
                       (&quot;Max depth = 5&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_depth = 5,
                                               oob_score = True)),
                       (&quot;Max depth = 10&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_depth = 10,
                                               oob_score = True))]

# Test effect of max leaf node
classifier_scenario4 = [(&quot;Max leaf node = 5&quot;,
                         RandomForestClassifier(warm_start = True,
                                                max_leaf_nodes = 5,
                                                oob_score = True)),
                       (&quot;Max leaf node = 10&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_leaf_nodes = 10,
                                               oob_score = True)),
                       (&quot;Max leaf node = 20&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_leaf_nodes = 20,
                                               oob_score = True)),
                       (&quot;Max leaf node = 40&quot;,
                        RandomForestClassifier(warm_start = True,
                                               max_leaf_nodes = 40,
                                               oob_score = True))]

# Test effect of max leaf node
classifier_scenario5 = [(&quot;Min samples split = 5&quot;,
                         RandomForestClassifier(warm_start = True,
                                                min_samples_split = 5,
                                                oob_score = True)),
                       (&quot;Min samples split = 10&quot;,
                        RandomForestClassifier(warm_start = True,
                                               min_samples_split = 10,
                                               oob_score = True)),
                       (&quot;Min samples split = 20&quot;,
                        RandomForestClassifier(warm_start = True,
                                               min_samples_split = 20,
                                               oob_score = True)),
                       (&quot;Min samples split = 40&quot;,
                        RandomForestClassifier(warm_start = True,
                                               min_samples_split = 40,
                                               oob_score = True))]

# Produce figures and export plots for each set of classification scenarios
for i, classifier_scenario in enumerate([classifier_scenario1,
                                         classifier_scenario2,
                                         classifier_scenario3,
                                         classifier_scenario4,
                                         classifier_scenario5]):

    # Plot OOB error by classifier scenario
    randomforest_eval(training_labels = train_lab,
                      training_samples = train_samp,
                      classifier_scenario = classifier_scenario,
                      output_path = &quot;figures/random_forest_params_{}.png&quot;.format(i + 1),
                      max_estimators = 200)

</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_15_0.png" src="../../_images/notebooks_algorithms_RandomForestClassification_15_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_15_1.png" src="../../_images/notebooks_algorithms_RandomForestClassification_15_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_15_2.png" src="../../_images/notebooks_algorithms_RandomForestClassification_15_2.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_15_3.png" src="../../_images/notebooks_algorithms_RandomForestClassification_15_3.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_algorithms_RandomForestClassification_15_4.png" src="../../_images/notebooks_algorithms_RandomForestClassification_15_4.png" />
</div>
</div>
</div>
<div class="section" id="Export-tree-diagrams">
<h1>Export tree diagrams<a class="headerlink" href="#Export-tree-diagrams" title="Permalink to this headline">¶</a></h1>
<p>Export .png plots of each decision tree in the random forest ensemble.
Useful for inspecting the splits used by the classifier to classify the
data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span># Plot output random forest trees to file
for n, tree_in_forest in enumerate(classifier.estimators_):

    # Create graph and save to dot file
    export_graphviz(tree_in_forest,
                    out_file = &quot;figures/tree_graphs/tree.dot&quot;,
                    feature_names = list(analysis_xarray.data_vars),
                    class_names = classification_names,
                    filled = True,
                    rounded = True)

    # Plot as figure
    os.system(&#39;dot -Tpng figures/tree_graphs/tree.dot -o &#39; + \
              &#39;figures/tree_graphs/tree&#39; + str(n + 1) + &#39;.png&#39;)

</pre></div>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../case_studies/README.html" class="btn btn-neutral float-right" title="Case Studies" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ProduceFalseColourGeotiffs.html" class="btn btn-neutral" title="ProduceFalseColourGeotiffs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Geoscience Australia.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1.0.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  


<!-- Theme Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113800428-1"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());
 
 gtag('config', 'UA-113800428-1');
</script>

    
  


</body>
</html>